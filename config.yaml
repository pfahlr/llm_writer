# rag_writer-lite model + provider config
version: 0.1

# Change this to whatever you want as your global default
default_model: or-mistral-small-3.2-24b
providers:
  openrouter:
    type: openrouter
    base_url: https://openrouter.ai/api/v1
    api_key_env: OPENROUTER_API_KEY
  googleg:
    type: google
    base_url: https://generativelanguage.googleapis.com/v1beta/models/
    api_key_env: GEMINI_API_KEY

models:
  # --- Mistral ---
  - id: gemini-2.5-pro
    label: "Gemini 2.5 Pro"
    system_prompt: | 
      You are a detail oriented assistant that delivers outlines, and written content in a variety of subject areas
    provider: google
    litellm_model: "gemini/gemini-pro"
    max_context_tokens: 1048576
    default_params:
      reasoning_effort: 'none'
    tags: ["google", "gemini", "advanced"]

  - id: gemini-1.5-pro
    label: "Gemini 2.5 Pro"
    system_prompt: |
      You are a detail oriented assistant that delivers outlines, and written content in a variety of subject areas
    provider: google
    litellm_model: "gemini/gemini-1.5-pro"
    max_context_tokens: 2000000
    default_params:
      reasoning_effort: 'none'
      max_tokens: 1048576
    tags: ["google", "gemini", "advanced"]

  - id: gemini-2.0-flash
    label: "Gemini 2.0 Flash"
    litellm_model: "gemini/gemini-2.0-flash"
    provider: google
    max_context_tokens: 1048576
    default_params:
      reasoning_effort: 'none'
      max_tokens: 1048576
    tags: ["google", "gemini", "advanced"]

  - id: gemini-2.0-flash-exp
    label: "Gemini 2.0 Flash Experimental"
    litellm_model: "gemini/gemini-2.0-flash-exp"
    provider: google
    max_context_tokens: 1048576
    default_params:
      reasoning_effort: 'none'
      max_tokens: 1048576
    tags: ["google", "gemini", "advanced"]

  - id: or-mistral-small-3.1-24b
    label: "Mistral Small 3.1 24B"
    system_prompt: |
      You are a planning-focused assistant that delivers organized outlines and key takeaways.
    provider: openrouter
    litellm_model: "openrouter/mistralai/mistral-small-3.1-24b-instruct"
    max_context_tokens: 128000
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["mistral", "general", "planning"]

  - id: or-mistral-small-3.2-24b
    label: "Mistral Small 3.2 24B"
    system_prompt: |
      You are a concise reasoning assistant who cites MCP findings clearly.
    provider: openrouter
    litellm_model: "openrouter/mistralai/mistral-small-3.2-24b-instruct"
    max_context_tokens: 131072
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["mistral", "general", "planning"]

  # --- OpenAI OSS ---

  - id: or-gpt-oss-20b
    label: "OpenAI GPT-OSS 20B"
    system_prompt: |
      You are a stylistic writing partner that preserves the user's intent and voice.
    provider: openrouter
    litellm_model: "openrouter/openai/gpt-oss-20b"
    max_context_tokens: 131072
    default_params:
      temperature: 0.15
      top_p: 0.95
      max_tokens: 4096
    tags: ["openai-oss", "general", "reasoning"]

  # --- Tongyi / Alibaba ---

  - id: or-tongyi-deepresearch-30b-a3b
    label: "Tongyi DeepResearch 30B A3B"
    system_prompt: |
      You are an exhaustive researcher that expands each idea with supporting evidence.
    provider: openrouter
    litellm_model: "openrouter/alibaba/tongyi-deepresearch-30b-a3b"
    max_context_tokens: 131072
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["reasoning", "research", "analysis"]

  # --- Qwen3 30B ---

  - id: or-qwen3-30b-a3b
    label: "Qwen3 30B A3B"
    system_prompt: |
      You are a structured coding and tooling assistant with a focus on readable solutions.
    provider: openrouter
    litellm_model: "openrouter/qwen/qwen3-30b-a3b"
    max_context_tokens: 131072
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["qwen3", "reasoning", "general"]

  # --- ArliAI / QwQ ---

  - id: or-qwq-32b-rpr-v1
    label: "ArliAI QwQ 32B RpR v1"
    provider: openrouter
    litellm_model: "openrouter/arliai/qwq-32b-arliai-rpr-v1"
    max_context_tokens: 32768
    default_params:
      temperature: 0.3
      top_p: 0.95
      max_tokens: 4096
    tags: ["roleplay", "creative", "general"]

  # --- Qwen2.5 VL ---

  - id: or-qwen2_5-vl-32b-instruct
    label: "Qwen2.5 VL 32B Instruct"
    provider: openrouter
    litellm_model: "openrouter/qwen/qwen2.5-vl-32b-instruct"
    max_context_tokens: 131072
    default_params:
      temperature: 0.25
      top_p: 0.95
      max_tokens: 4096
    tags: ["vision", "multimodal", "general"]

  # --- DeepSeek V3 & V3.1 ---

  - id: or-deepseek-v3-0324
    label: "DeepSeek V3 0324"
    provider: openrouter
    litellm_model: "openrouter/deepseek/deepseek-chat-v3-0324"
    max_context_tokens: 164000
    default_params:
      temperature: 0.15
      top_p: 0.9
      max_tokens: 4096
    tags: ["deepseek", "reasoning", "general"]

  - id: or-deepseek-v3_1
    label: "DeepSeek V3.1"
    system_prompt: |
      You are a calm analytical tutor who narrates intermediate reasoning steps.
    provider: openrouter
    litellm_model: "openrouter/deepseek/deepseek-chat-v3.1"
    max_context_tokens: 163800
    default_params:
      temperature: 0.15
      top_p: 0.9
      max_tokens: 4096
    tags: ["deepseek", "reasoning", "long-context"]

  # --- Gemma 3 27B ---

  - id: or-gemma-3-27b
    label: "Gemma 3 27B"
    system_prompt: |
      You are an empathetic writing mentor that structures outlines cleanly.
    provider: openrouter
    litellm_model: "openrouter/google/gemma-3-27b-it"
    max_context_tokens: 128000
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["gemma3", "general", "multilingual"]

  # --- Z.AI GLM 4.5 Air ---

  - id: or-glm-4_5-air
    label: "GLM 4.5 Air"
    system_prompt: |
      You are an execution planner who maps milestones, risks, and dependencies.
    provider: openrouter
    litellm_model: "openrouter/z-ai/glm-4.5-air"
    max_context_tokens: 131072
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["glm", "reasoning", "agentic"]

  # --- TNG DeepSeek Chimera Models ---

  - id: or-deepseek-r1t-chimera
    label: "DeepSeek R1T Chimera"
    provider: openrouter
    litellm_model: "openrouter/tngtech/deepseek-r1t-chimera"
    max_context_tokens: 163840
    default_params:
      temperature: 0.15
      top_p: 0.9
      max_tokens: 4096
    tags: ["deepseek", "chimera", "reasoning"]

  - id: or-deepseek-r1t2-chimera
    label: "DeepSeek R1T2 Chimera"
    provider: openrouter
    litellm_model: "openrouter/tngtech/deepseek-r1t2-chimera"
    max_context_tokens: 130000
    default_params:
      temperature: 0.15
      top_p: 0.9
      max_tokens: 4096
    tags: ["deepseek", "chimera", "reasoning", "fast"]

  # --- MoonshotAI Kimi K2 0711 ---

  - id: or-kimi-k2
    label: "Kimi K2 0711"
    system_prompt: |
      You are a precision summarizer tailored for executive briefs.
    provider: openrouter
    litellm_model: "openrouter/moonshotai/kimi-k2"
    max_context_tokens: 128000
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["kimi-k2", "agentic", "coding"]

  # --- Qwen3 235B A22B ---

  - id: or-qwen3-235b-a22b
    label: "Qwen3 235B A22B"
    system_prompt: |
      You are a meticulous deep-reasoning agent comfortable with research analysis.
    provider: openrouter
    litellm_model: "openrouter/qwen/qwen3-235b-a22b"
    max_context_tokens: 131072
    default_params:
      temperature: 0.2
      top_p: 0.95
      max_tokens: 4096
    tags: ["qwen3", "reasoning", "long-context"]

  # --- Qwen3 Coder 480B A35B ---

  - id: or-qwen3-coder-480b-a35b
    label: "Qwen3 Coder 480B A35B"
    provider: openrouter
    litellm_model: "openrouter/qwen/qwen3-coder"
    max_context_tokens: 262144
    default_params:
      temperature: 0.1
      top_p: 0.9
      max_tokens: 4096
    tags: ["qwen3", "coder", "agentic", "coding"]

  # --- Venice: Uncensored ---

  - id: or-venice-uncensored
    label: "Venice: Uncensored"
    provider: openrouter
    litellm_model: "openrouter/venice/uncensored"
    max_context_tokens: 32768
    default_params:
      temperature: 0.4
      top_p: 0.98
      max_tokens: 4096
    tags: ["venice", "uncensored", "creative"]

llm_tool:
  id: "llm"
  tool_name: "llm-complete"
  title: "LLM skill completions"
  description: "Expose a curated skill set as an MCP tool."
  default_skill: "reason"
  max_tokens_limit: 4096
  skills:
    - id: reason
      label: "Research Reasoner"
      description: "Deep research analysis with citations."
      model_id: or-qwen3-235b-a22b
      prompt_id: reasoning_research
      max_output_tokens: 4096
      temperature: 0.3
    - id: summarize
      label: "Concise Summarizer"
      description: "Executive summaries optimized for briefs."
      model_id: or-kimi-k2
      prompt_id: summary_compact
      max_output_tokens: 1500
      temperature: 0.15
    - id: solve
      label: "Structured Problem Solver"
      description: "Step-by-step reasoning tutor."
      model_id: or-deepseek-v3_1
      prompt_id: solver_breakdown
      max_output_tokens: 2048
      temperature: 0.1
    - id: write
      label: "Outline Architect"
      description: "Generates book/article outlines and section plans."
      model_id: or-gemma-3-27b
      prompt_id: outline_architect
      max_output_tokens: 3000
      temperature: 0.35
    - id: write2
      label: "Creative Voice Crafter"
      description: "Creative drafts and voice-preserving rewrites."
      model_id: or-gpt-oss-20b
      prompt_id: creative_voice
      max_output_tokens: 2200
      temperature: 0.5
    - id: plan
      label: "Macro Planner"
      description: "Creates phased execution plans."
      model_id: or-glm-4_5-air
      prompt_id: planner_macro
      max_output_tokens: 2600
      temperature: 0.2
    - id: code
      label: "Coding Copilot"
      description: "Implementation sketches and refactors."
      model_id: or-qwen3-30b-a3b
      prompt_id: coding_helper
      max_output_tokens: 1800
      temperature: 0.1
    - id: extract_data
      label: "Extraction Specialist"
      description: "Turns passages into structured JSON."
      model_id: or-mistral-small-3.1-24b
      prompt_id: extraction_specialist
      max_output_tokens: 1600
      temperature: 0.1

mcp_servers:
  - id: arxiv
    command:
      - "uv"
      - "tool"
      - "run"
      - "arxiv-mcp-server"
      - "--storage-path"
      - "/home/rick/.cache/arxiv-papers"
    auto_start: true
  - id: "llm"
    command:
      - "python"
      - "srw_llm_tool.py"
      - "--config"
      - "config.yaml"
    auto_start: true
  - id: "dre"
    command:
      - "npx"
      - "deliberate-reasoning-engine"
    auto_start: true
  - id: memory_bank_mcp
    command:
      - "npx"
      - "@movibe/memory-bank-mcp"
      - "--mode ask"
    auto_start: true
  - id: cyberchitta-llm-context
    command:
      - "uvx"
      - "--from"
      - "llm-context"
      - "lc-mcp"
    auto_start: true
  - id: rushdb
    command:
     - "npx"
     - "@rushdb/mcp-server"

