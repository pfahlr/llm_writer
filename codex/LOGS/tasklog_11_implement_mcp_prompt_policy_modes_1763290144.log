OpenAI Codex v0.57.0 (research preview)
--------
workdir: /var/home/rick/Development/llm_writer
model: gpt-5-codex
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: high
reasoning summaries: auto
session id: 019a8c48-60ca-7fd3-9895-4f8d250947a8
--------
user
Implement the functionality described in codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml, then verify with any available tests.

thinking
**Starting with basic exploration**
exec
bash -lc pwd in /var/home/rick/Development/llm_writer succeeded in 37ms:
/home/rick/Development/llm_writer

thinking
**Listing tasks directory**
exec
bash -lc 'ls codex/TASKS' in /home/rick/Development/llm_writer succeeded in 41ms:
01_bootstrap_project_and_cli.yaml
02_implement_config_models_and_loader.yaml
03_implement_model_registry_and_litellm_integration.yaml
04_define_task_spec_and_loader.yaml
05_implement_prompt_builders.yaml
06_implement_mcp_types_and_normalization.yaml
07_implement_automated_task_runner.yaml
08_implement_planning_log_writer.yaml
09_implement_planning_repl.yaml
10_implement_replay_support.yaml
11_implement_mcp_prompt_policy_modes.yaml
12_add_textual_source_browser_stub.yaml
CODEX_TODO.md
master_spec.yaml
work_list

thinking
**Reading instructions and task file**
exec
bash -lc 'cat codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml' in /home/rick/Development/llm_writer succeeded in 37ms:
id: 11
filename: 11_implement_mcp_prompt_policy_modes.yaml
title: Implement MCP prompt policy modes (raw_capped, summary)
description: >
  Implement the logic to apply the configured MCP prompt policy to
  NormalizedItems before injecting them into task prompts, including
  raw_capped and summary modes.

dependencies:
  - 06
  - 07

goals:
  - AppConfig.mcp_prompt_policy.default_mode is honored.
  - raw_capped mode enforces max_items_per_reference, max_chars_per_item,
    and max_total_chars.
  - summary mode uses a summarizer model and per-type prompts.

artifacts:
  - src/simple_rag_writer/mcp/prompt_policy.py

thinking
**Outlining multi-step plan**
Plan update
Implementing MCP prompt policy requires coordinating config, new module, and runner changes; using a plan ensures TDD and coverage.
  → Review existing config/models/tests to understand MCP prompt policy requirements; identify relevant structures and entry points.
  • Write failing tests for raw_capped and summary prompt policy behavior using fake ModelRegistry.
  • Implement prompt_policy module and integrate with runner to satisfy tests; verify by running pytest.

thinking
**Searching for prompt policy configuration**
exec
bash -lc 'rg -n "prompt_policy" -n' in /home/rick/Development/llm_writer succeeded in 43ms:
README.md:41:mcp_prompt_policy:
tests/test_runner_run_tasks.py:47:    mcp_prompt_policy=McpPromptPolicy(
tests/test_config_loader.py:35:    mcp_prompt_policy:
tests/test_config_loader.py:64:  assert cfg.mcp_prompt_policy.default_mode == "summary"
tests/test_config_loader.py:89:def test_load_config_rejects_invalid_prompt_policy_mode(tmp_path: Path) -> None:
tests/test_config_loader.py:100:    mcp_prompt_policy:
src/simple_rag_writer/runner/run_tasks.py:116:def _apply_prompt_policy(
src/simple_rag_writer/runner/run_tasks.py:122:  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
src/simple_rag_writer/runner/run_tasks.py:124:    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
src/simple_rag_writer/runner/run_tasks.py:125:  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
src/simple_rag_writer/runner/run_tasks.py:177:          blob = _apply_prompt_policy(ref, items, config, registry)
src/simple_rag_writer/runner/run_tasks.py:192:          blob = _apply_prompt_policy(ref, items, config, registry)
src/simple_rag_writer/config/models.py:67:  mcp_prompt_policy: McpPromptPolicy = Field(default_factory=McpPromptPolicy)
codex/TASKS/work_list:11:11_implement_mcp_prompt_policy_modes.yaml
codex/TASKS/CODEX_TODO.md:15:- [ ] 11_implement_mcp_prompt_policy_modes.yaml
codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml:2:filename: 11_implement_mcp_prompt_policy_modes.yaml
codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml:14:  - AppConfig.mcp_prompt_policy.default_mode is honored.
codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml:20:  - src/simple_rag_writer/mcp/prompt_policy.py
codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml:22:  - tests/test_mcp_prompt_policy.py
codex/TASKS/11_implement_mcp_prompt_policy_modes.yaml:25:  - Create mcp/prompt_policy.py with apply_prompt_policy(app_config, items,

thinking
**Inspecting run_tasks code**
exec
bash -lc "sed -n '1,260p' src/simple_rag_writer/runner/run_tasks.py" in /home/rick/Development/llm_writer succeeded in 38ms:
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from rich.console import Console

from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
from simple_rag_writer.llm.registry import ModelRegistry
from simple_rag_writer.mcp.client import McpClient
from simple_rag_writer.mcp.normalization import normalize_payload
from simple_rag_writer.mcp.types import NormalizedItem
from simple_rag_writer.prompts.task_prompt import build_task_prompt
from simple_rag_writer.runner.url_fetcher import fetch_url_text
from simple_rag_writer.tasks.loader import expand_task_paths, load_task
from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference

console = Console()
DEFAULT_SUMMARY_PROMPT = (

thinking
**Identifying prompt policy handling**
exec
bash -lc "sed -n '1,260p' src/simple_rag_writer/config/models.py" in /home/rick/Development/llm_writer succeeded in 38ms:
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Literal, Optional

from pydantic import BaseModel, Field


class ProviderConfig(BaseModel):
  type: str
  api_key_env: Optional[str] = None
  api_key: Optional[str] = None
  base_url: Optional[str] = None
  model_prefix: Optional[str] = None


class ModelConfig(BaseModel):
  id: str
  provider: str
  model_name: str

thinking
**Preparing to create policy-related module and tests**
exec
bash -lc "sed -n '1,260p' tests/test_runner_run_tasks.py" in /home/rick/Development/llm_writer succeeded in 40ms:
from __future__ import annotations

from io import StringIO
from pathlib import Path
from textwrap import dedent

from rich.console import Console

from simple_rag_writer.config.models import (
  AppConfig,
  McpPromptPolicy,
  ModelConfig,
  ProviderConfig,
  RawCappedPolicy,
  SummaryPolicy,
)
from simple_rag_writer.mcp.types import McpToolResult
from simple_rag_writer.runner.run_tasks import run_tasks_for_paths



thinking
**Checking for prompt policy module**
exec
bash -lc 'ls src/simple_rag_writer/mcp' in /home/rick/Development/llm_writer succeeded in 38ms:
client.py
__init__.py
normalization.py
__pycache__
types.py

thinking
**Planning prompt_policy module and tests**
exec
bash -lc "sed -n '1,240p' src/simple_rag_writer/mcp/types.py" in /home/rick/Development/llm_writer succeeded in 39ms:
from __future__ import annotations

from typing import Any, Dict, Optional

from pydantic import BaseModel, Field


class McpToolResult(BaseModel):
  server_id: str
  tool_name: str
  payload: Any


class NormalizedItem(BaseModel):
  id: Optional[str] = None
  type: Optional[str] = None
  title: Optional[str] = None
  snippet: Optional[str] = None
  body: Optional[str] = None
  url: Optional[str] = None

thinking
**Inspecting prompt policy tests**
exec
bash -lc "sed -n '1,320p' src/simple_rag_writer/mcp/normalization.py" in /home/rick/Development/llm_writer succeeded in 39ms:
from __future__ import annotations

from typing import Any, Dict, List, Optional

from .types import NormalizedItem


def normalize_payload(
  payload: Any,
  item_type_hint: Optional[str] = None,
) -> List[NormalizedItem]:
  """Convert raw MCP payload into a list of NormalizedItem."""
  items: List[NormalizedItem] = []

  if isinstance(payload, str):
    items.append(NormalizedItem(body=payload, type=item_type_hint))
  elif isinstance(payload, list):
    for idx, entry in enumerate(payload):
      if isinstance(entry, str):
        items.append(NormalizedItem(id=str(idx), body=entry, type=item_type_hint))

thinking
**Extracting prompt policy to module**
exec
bash -lc "sed -n '780,900p' codex/TASKS/master_spec.yaml" in /home/rick/Development/llm_writer succeeded in 38ms:
        - Optionally set initial model from CLI.
        - Enter REPL loop:
          - If line starts with '/', interpret as command:
            - /models, /model, /sources, /use, /inject, /quit.
          - Else:
            - start_turn(N, user_text) on log writer.
            - Build planning prompt using:
              - System instructions.
              - MCP context buffer (if any).
              - Recent history (last N turns).
              - Current user message.
            - Call ModelRegistry.complete with current model + merged params.
            - log_model_used(current_model_id).
            - If /inject occurred, log_mcp_injection(N, items) with normalized
              MCP data for that turn.
            - end_turn(N, assistant_text) on log writer.
            - Append (user, assistant) pair to history.
        - On exit, call log_writer.close().
    automated_mode:
      steps:

thinking
**Preparing to implement and test new module**
exec
bash -lc "sed -n '1,320p' src/simple_rag_writer/llm/registry.py" in /home/rick/Development/llm_writer succeeded in 44ms:
from __future__ import annotations

import os
from typing import Any, Dict, List, Optional

try:
  import litellm
except ImportError:  # pragma: no cover - only triggered when dependency missing
  litellm = None

from simple_rag_writer.config.models import AppConfig, ModelConfig, ProviderConfig
from .params import merge_generation_params


class ModelRegistry:
  def __init__(self, config: AppConfig):
    self._config = config
    self._models: Dict[str, ModelConfig] = {m.id: m for m in config.models}
    if config.default_model not in self._models:
      raise ValueError(f"default_model {config.default_model} not found in models")

thinking
**Clarifying apply_prompt_policy return type**
exec
bash -lc 'rg -n "raw_capped" -n' in /home/rick/Development/llm_writer succeeded in 48ms:
README.md:42:  default_mode: "raw_capped"
tests/test_runner_run_tasks.py:48:      default_mode="raw_capped",
tests/test_runner_run_tasks.py:49:      raw_capped=RawCappedPolicy(
tests/test_config_loader.py:37:      raw_capped:
src/simple_rag_writer/runner/run_tasks.py:125:  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
src/simple_rag_writer/config/models.py:39:  default_mode: Literal["raw_capped", "summary"] = "raw_capped"
src/simple_rag_writer/config/models.py:40:  raw_capped: RawCappedPolicy = Field(default_factory=RawCappedPolicy)
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:47:title: Implement MCP prompt policy modes (raw_capped, summary)
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:51:  raw_capped and summary modes.
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:59:  - raw_capped mode enforces max_items_per_reference, max_chars_per_item,
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:71:  • Write failing tests for raw_capped and summary prompt policy behavior using fake ModelRegistry.
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:87:src/simple_rag_writer/runner/run_tasks.py:125:  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:61:    - MCP prompt policy (raw_capped, summary).
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:376:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:484:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:591:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:725:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:857:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:989:+      raw_capped:
codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log:1118:+      raw_capped:

thinking
**Opening README for prompt policy snippet**
exec
bash -lc "sed -n '1,200p' README.md" in /home/rick/Development/llm_writer succeeded in 42ms:
# Simple Rag Writer

Early scaffold for a slim writing assistant that:
- Uses multiple LLM providers (OpenAI, OpenRouter, Gemini) via `litellm`.
- Uses MCP servers as its only retrieval/knowledge layer.
- Supports interactive planning (`srw -c config.yaml plan`).
- Supports automated task execution from YAML (`srw -c config.yaml run tasks/*.yaml`).
- Supports replaying planning prompts from logs (`srw -c config.yaml replay --log ... --turn ...`).

This is an incomplete scaffold intended for test-driven development.
Many components are stubs with TODOs.

## Getting Started

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
srw --help
srw -c config.yaml plan

thinking
**Updating plan status**
Plan update
  ✓ Review existing config/models/tests to understand MCP prompt policy requirements; identify relevant structures and entry points.
  → Write failing tests for raw_capped and summary prompt policy behavior using fake ModelRegistry.
  • Implement prompt_policy module and integrate with runner to satisfy tests; verify by running pytest.

thinking
**Planning test design**
exec
bash -lc "sed -n '1,320p' src/simple_rag_writer/tasks/models.py" in /home/rick/Development/llm_writer succeeded in 40ms:
from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class ContextSpec(BaseModel):
  outline_path: Optional[str] = None
  outline_id: Optional[str] = None


class ReferenceCommon(BaseModel):
  type: str
  label: Optional[str] = None
  item_type: Optional[str] = None
  prompt_mode: Optional[str] = None
  max_items: Optional[int] = None
  max_chars: Optional[int] = None
  summary_max_tokens: Optional[int] = None


thinking
**Designing tests for prompt policy behaviors**

I'm outlining detailed test plans to verify both raw_capped and summary modes respect limits, defaults, and prompt usage. I'll create a fake model registry and minimal config helpers to simulate scenarios, ensuring tests confirm truncation, prompt selection, and summarizer calls all align with intended behavior. Let's explore implementing these to enable TDD for the new prompt policy module!
thinking
**Preparing test cases for prompt mode overrides and summaries**

I’m planning detailed pytest functions to verify that the prompt policy correctly applies raw_capped limits and summary behavior, including reference prompt_mode overrides, per-type prompts, and max item/token restrictions. I will implement a FakeModelRegistry for capturing calls and responses, and set up minimal config helpers to streamline test setup and assertions on truncation and summarization calls.
file update
A /var/home/rick/Development/llm_writer/tests/test_mcp_prompt_policy.py
from __future__ import annotations

from simple_rag_writer.config.models import (
  AppConfig,
  McpPromptPolicy,
  ModelConfig,
  ProviderConfig,
  RawCappedPolicy,
  SummaryPolicy,
)
from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
from simple_rag_writer.mcp.types import NormalizedItem
from simple_rag_writer.tasks.models import McpReference


class FakeModelRegistry:
  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
    self.response = response
    self.calls = []

  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
    self.calls.append(
      {
        "prompt": prompt,
        "model_id": model_id,
        "task_params": task_params,
      }
    )
    return self.response


def _make_config() -> AppConfig:
  return AppConfig(
    default_model="writer-model",
    providers={
      "local": ProviderConfig(type="openai", api_key="dummy"),
    },
    models=[
      ModelConfig(id="writer-model", provider="local", model_name="writer"),
      ModelConfig(id="summary-model", provider="local", model_name="summary"),
    ],
    mcp_prompt_policy=McpPromptPolicy(
      default_mode="raw_capped",
      raw_capped=RawCappedPolicy(
        max_items_per_reference=2,
        max_chars_per_item=5,
        max_total_chars=9,
      ),
      summary=SummaryPolicy(
        summarizer_model="summary-model",
        max_items_per_reference=3,
        summary_max_tokens=128,
        default_prompt="Default summary prompt.",
        per_type_prompts={
          "case": "Summarize the legal case evidence.",
        },
      ),
    ),
  )


def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
  config = _make_config()
  registry = FakeModelRegistry()
  reference = McpReference(server="notes", tool="search")
  items = [
    NormalizedItem(body="AAAAA11111"),
    NormalizedItem(body="BBBBB22222"),
    NormalizedItem(body="CCCCC33333"),
  ]

  text = apply_prompt_policy(config, items, reference, registry)

  assert text == "AAAAA\n\nBBBB"
  assert registry.calls == []


def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
  config = _make_config()
  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
  reference = McpReference(
    server="notes",
    tool="search",
    prompt_mode="summary",
    item_type="case",
    summary_max_tokens=42,
    max_items=1,
  )
  items = [
    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
    NormalizedItem(title="Doc 2", body="Beta body"),
  ]

  text = apply_prompt_policy(config, items, reference, registry)

  assert text == "SUMMARY TEXT"
  assert len(registry.calls) == 1
  call = registry.calls[0]
  assert call["model_id"] == "summary-model"
  assert call["task_params"] == {"max_tokens": 42}
  assert "Summarize the legal case evidence." in call["prompt"]
  assert "Doc 1" in call["prompt"]
  assert "Doc 2" not in call["prompt"]
apply_patch(auto_approved=true) exited 0 in 4ms:
Success. Updated the following files:
A tests/test_mcp_prompt_policy.py
file update:
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]

file update:
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Executing targeted pytest run**
exec
bash -lc 'pytest tests/test_mcp_prompt_policy.py' in /home/rick/Development/llm_writer exited 2 in 1.49s:
============================= test session starts ==============================
platform linux -- Python 3.13.9, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/llm_writer
configfile: pyproject.toml
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error

==================================== ERRORS ====================================
_______________ ERROR collecting tests/test_mcp_prompt_policy.py _______________
ImportError while importing test module '/var/home/rick/Development/llm_writer/tests/test_mcp_prompt_policy.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/lib64/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/test_mcp_prompt_policy.py:11: in <module>
    from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
E   ModuleNotFoundError: No module named 'simple_rag_writer.mcp.prompt_policy'
=========================== short test summary info ============================
file update:
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Updating plan and progress**
Plan update
  ✓ Review existing config/models/tests to understand MCP prompt policy requirements; identify relevant structures and entry points.
  ✓ Write failing tests for raw_capped and summary prompt policy behavior using fake ModelRegistry.
  → Implement prompt_policy module and integrate with runner to satisfy tests; verify by running pytest.
file update:
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Planning prompt policy refactor**

I’m planning to create a new module `prompt_policy.py` to centralize prompt policy logic currently duplicated in the runner. This involves moving functions like `_format_item` and `_render_raw_blob` while ensuring proper imports, type annotations, and handling all policy limits. Then I'll update the runner to use this new module and clean up duplicates. I'll also check tests and other references to keep everything consistent.
exec
bash -lc 'rg -n "_apply_prompt_policy" -n' in /home/rick/Development/llm_writer succeeded in 45ms:
tests/test_mcp_prompt_policy.py:62:def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
tests/test_mcp_prompt_policy.py:78:def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
src/simple_rag_writer/runner/run_tasks.py:116:def _apply_prompt_policy(
src/simple_rag_writer/runner/run_tasks.py:177:          blob = _apply_prompt_policy(ref, items, config, registry)
src/simple_rag_writer/runner/run_tasks.py:192:          blob = _apply_prompt_policy(ref, items, config, registry)
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:84:src/simple_rag_writer/runner/run_tasks.py:116:def _apply_prompt_policy(
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:88:src/simple_rag_writer/runner/run_tasks.py:177:          blob = _apply_prompt_policy(ref, items, config, registry)
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:89:src/simple_rag_writer/runner/run_tasks.py:192:          blob = _apply_prompt_policy(ref, items, config, registry)
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:438:def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:454:def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:551:+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:567:+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:662:+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:678:+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:798:+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:814:+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:916:+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:932:+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
codex/LOGS/tasklog_11_implement_mcp_prompt_policy_modes_1763290144.log:965:bash -lc 'rg -n "_apply_prompt_policy" -n' in /home/rick/Development/llm_writer
codex/LOGS/tasklog_07_implement_automated_task_runner_1763288534.log:2993:+def _apply_prompt_policy(
file update:
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Preparing prompt policy refactor**

I'm planning to move summary and raw rendering functions out of the runner into a new module called `prompt_policy`, carefully replicating logic and constants like `DEFAULT_SUMMARY_PROMPT` there. Then I'll update the runner to import and use the new module's `apply_prompt_policy` function, cleaning up unused code and adjusting imports accordingly. This will keep things organized and maintain existing behavior.
file update
A /var/home/rick/Development/llm_writer/src/simple_rag_writer/mcp/prompt_policy.py
from __future__ import annotations

from typing import List, Optional

from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
from simple_rag_writer.llm.registry import ModelRegistry
from simple_rag_writer.mcp.types import NormalizedItem
from simple_rag_writer.tasks.models import ReferenceCommon

DEFAULT_SUMMARY_PROMPT = (
  "Summarize the following reference material into key bullet points, highlighting"
  " actionable facts and important context."
)


def _format_item(item: NormalizedItem) -> str:
  parts: List[str] = []
  if item.title:
    parts.append(f"Title: {item.title}")
  if item.url:
    parts.append(f"URL: {item.url}")
  body = (item.body or item.snippet or "").strip()
  if body:
    parts.append(body)
  elif item.metadata:
    parts.append(str(item.metadata))
  return "\n".join(parts).strip()


def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
  if not items:
    return None
  per_item_limit = reference.max_chars or policy.max_chars_per_item
  total_limit = policy.max_total_chars
  max_items = reference.max_items or policy.max_items_per_reference
  chunks: List[str] = []
  used = 0
  for item in items[:max_items]:
    formatted = _format_item(item)
    if not formatted:
      continue
    snippet = formatted[:per_item_limit].strip()
    if not snippet:
      continue
    if used + len(snippet) > total_limit:
      remaining = total_limit - used
      if remaining <= 0:
        break
      snippet = snippet[:remaining].rstrip()
    chunks.append(snippet)
    used += len(snippet)
    if used >= total_limit:
      break
  if not chunks:
    return None
  return "\n\n".join(chunks)


def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
  template = None
  if ref_type:
    template = policy.per_type_prompts.get(ref_type)
  if not template:
    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
  sections: List[str] = []
  for idx, item in enumerate(items, start=1):
    heading = item.title or f"Item {idx}"
    body = (item.body or item.snippet or "").strip()
    lines = [f"## {heading}"]
    if body:
      lines.append(body)
    if item.url:
      lines.append(f"Source: {item.url}")
    sections.append("\n".join(lines).strip())
  body_text = "\n\n".join(sections)
  return f"{template.strip()}\n\n{body_text}".strip()


def _render_summary_blob(
  reference: ReferenceCommon,
  items: List[NormalizedItem],
  policy: SummaryPolicy,
  registry: ModelRegistry,
) -> Optional[str]:
  if not items:
    return None
  max_items = reference.max_items or policy.max_items_per_reference
  limited = items[:max_items]
  prompt = _build_summary_prompt(reference, limited, policy)
  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
  summary = registry.complete(
    prompt,
    model_id=policy.summarizer_model,
    task_params={"max_tokens": max_tokens},
  )
  return summary.strip() or None


def apply_prompt_policy(
  config: AppConfig,
  items: List[NormalizedItem],
  reference: ReferenceCommon,
  registry: ModelRegistry,
) -> Optional[str]:
  if not items:
    return None
  policy = config.mcp_prompt_policy
  mode = reference.prompt_mode or policy.default_mode
  if mode == "summary":
    return _render_summary_blob(reference, items, policy.summary, registry)
  return _render_raw_blob(reference, items, policy.raw_capped)
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
A src/simple_rag_writer/mcp/prompt_policy.py
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]

file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Preparing import and usage updates**
file update
M /var/home/rick/Development/llm_writer/src/simple_rag_writer/runner/run_tasks.py
@@ -8,3 +8,3 @@
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
@@ -12,2 +12,3 @@
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
@@ -19,6 +20,2 @@
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
@@ -29,100 +26,4 @@
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
@@ -176,3 +77,3 @@
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
@@ -191,3 +92,3 @@
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M src/simple_rag_writer/runner/run_tasks.py
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]

file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Verifying imports and test readiness**
exec
bash -lc 'pytest tests/test_mcp_prompt_policy.py' in /home/rick/Development/llm_writer succeeded in 1.19s:
============================= test session starts ==============================
platform linux -- Python 3.13.9, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/llm_writer
configfile: pyproject.toml
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items

tests/test_mcp_prompt_policy.py ..                                       [100%]

============================== 2 passed in 0.42s ===============================
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Running full test suite**
exec
bash -lc pytest in /home/rick/Development/llm_writer succeeded in 1.44s:
============================= test session starts ==============================
platform linux -- Python 3.13.9, pytest-8.4.2, pluggy-1.6.0
rootdir: /var/home/rick/Development/llm_writer
configfile: pyproject.toml
testpaths: tests
plugins: Faker-37.8.0, xdist-3.8.0, mock-3.15.1, metadata-3.1.1, cov-7.0.0, asyncio-1.2.0, html-4.1.1, hypothesis-6.141.1, anyio-4.8.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 33 items

tests/test_cli_parser.py ..                                              [  6%]
tests/test_config_loader.py ...                                          [ 15%]
tests/test_llm_registry.py ....                                          [ 27%]
tests/test_mcp_normalization.py .....                                    [ 42%]
tests/test_mcp_prompt_policy.py ..                                       [ 48%]
tests/test_planning_log_writer.py ...                                    [ 57%]
tests/test_planning_repl.py ....                                         [ 69%]
tests/test_prompt_building.py ..                                         [ 75%]
tests/test_replay_reconstruct.py ...                                     [ 84%]
tests/test_runner_run_tasks.py ..                                        [ 90%]
tests/test_task_loading.py ...                                           [100%]
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Updating plan completion**
Plan update
  ✓ Review existing config/models/tests to understand MCP prompt policy requirements; identify relevant structures and entry points.
  ✓ Write failing tests for raw_capped and summary prompt policy behavior using fake ModelRegistry.
  ✓ Implement prompt_policy module and integrate with runner to satisfy tests; verify by running pytest.
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Checking changed files**
exec
bash -lc 'git status -sb' in /home/rick/Development/llm_writer succeeded in 40ms:
## main
?? codex/LOGS/codex_run_2025-11-16_04-58-12.log
?? codex/LOGS/tasklog_01_bootstrap_project_and_cli_1763287092.log
?? codex/LOGS/tasklog_01_bootstrap_project_and_cli_1763287092.log.done
?? codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log
?? codex/LOGS/tasklog_02_implement_config_models_and_loader_1763287503.log.done
?? codex/LOGS/tasklog_03_implement_model_registry_and_litellm_integration_1763287725.log
?? codex/LOGS/tasklog_03_implement_model_registry_and_litellm_integration_1763287725.log.done
?? codex/LOGS/tasklog_04_define_task_spec_and_loader_1763287876.log
?? codex/LOGS/tasklog_04_define_task_spec_and_loader_1763287876.log.done
?? codex/LOGS/tasklog_05_implement_prompt_builders_1763288031.log
?? codex/LOGS/tasklog_05_implement_prompt_builders_1763288031.log.done
?? codex/LOGS/tasklog_06_implement_mcp_types_and_normalization_1763288389.log
?? codex/LOGS/tasklog_06_implement_mcp_types_and_normalization_1763288389.log.done
?? codex/LOGS/tasklog_07_implement_automated_task_runner_1763288534.log
?? codex/LOGS/tasklog_07_implement_automated_task_runner_1763288534.log.done
?? codex/LOGS/tasklog_08_implement_planning_log_writer_1763289052.log
?? codex/LOGS/tasklog_08_implement_planning_log_writer_1763289052.log.done
?? codex/LOGS/tasklog_09_implement_planning_repl_1763289341.log
?? codex/LOGS/tasklog_09_implement_planning_repl_1763289341.log.done
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Preparing final change summary**
exec
bash -lc 'nl -ba src/simple_rag_writer/mcp/prompt_policy.py' in /home/rick/Development/llm_writer succeeded in 38ms:
     1	from __future__ import annotations
     2	
     3	from typing import List, Optional
     4	
     5	from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
     6	from simple_rag_writer.llm.registry import ModelRegistry
     7	from simple_rag_writer.mcp.types import NormalizedItem
     8	from simple_rag_writer.tasks.models import ReferenceCommon
     9	
    10	DEFAULT_SUMMARY_PROMPT = (
    11	  "Summarize the following reference material into key bullet points, highlighting"
    12	  " actionable facts and important context."
    13	)
    14	
    15	
    16	def _format_item(item: NormalizedItem) -> str:
    17	  parts: List[str] = []
    18	  if item.title:
    19	    parts.append(f"Title: {item.title}")
    20	  if item.url:
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Preparing line references for runner changes**
exec
bash -lc "nl -ba src/simple_rag_writer/runner/run_tasks.py | sed -n '1,260p'" in /home/rick/Development/llm_writer succeeded in 43ms:
     1	from __future__ import annotations
     2	
     3	from dataclasses import dataclass
     4	from pathlib import Path
     5	from typing import List, Optional
     6	
     7	from rich.console import Console
     8	
     9	from simple_rag_writer.config.models import AppConfig
    10	from simple_rag_writer.llm.registry import ModelRegistry
    11	from simple_rag_writer.mcp.client import McpClient
    12	from simple_rag_writer.mcp.normalization import normalize_payload
    13	from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
    14	from simple_rag_writer.mcp.types import NormalizedItem
    15	from simple_rag_writer.prompts.task_prompt import build_task_prompt
    16	from simple_rag_writer.runner.url_fetcher import fetch_url_text
    17	from simple_rag_writer.tasks.loader import expand_task_paths, load_task
    18	from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
    19	
    20	console = Console()
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Noting file locations for context**
exec
bash -lc 'nl -ba tests/test_mcp_prompt_policy.py' in /home/rick/Development/llm_writer succeeded in 38ms:
     1	from __future__ import annotations
     2	
     3	from simple_rag_writer.config.models import (
     4	  AppConfig,
     5	  McpPromptPolicy,
     6	  ModelConfig,
     7	  ProviderConfig,
     8	  RawCappedPolicy,
     9	  SummaryPolicy,
    10	)
    11	from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
    12	from simple_rag_writer.mcp.types import NormalizedItem
    13	from simple_rag_writer.tasks.models import McpReference
    14	
    15	
    16	class FakeModelRegistry:
    17	  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
    18	    self.response = response
    19	    self.calls = []
    20	
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Preparing final task summary**
exec
bash -lc 'git add src/simple_rag_writer/mcp/prompt_policy.py src/simple_rag_writer/runner/run_tasks.py tests/test_mcp_prompt_policy.py' in /home/rick/Development/llm_writer succeeded in 40ms:

file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Committing feature implementation**
exec
bash -lc 'git commit -m "feat: implement mcp prompt policy modes"' in /home/rick/Development/llm_writer succeeded in 42ms:
[main a6a2322] feat: implement mcp prompt policy modes
 3 files changed, 326 insertions(+)
 create mode 100644 src/simple_rag_writer/mcp/prompt_policy.py
 create mode 100644 src/simple_rag_writer/runner/run_tasks.py
 create mode 100644 tests/test_mcp_prompt_policy.py
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]


thinking
**Summarizing commit and next steps**
codex
Commit message: feat: implement mcp prompt policy modes

Modified files:
- src/simple_rag_writer/mcp/prompt_policy.py
- src/simple_rag_writer/runner/run_tasks.py
- tests/test_mcp_prompt_policy.py

- Added `apply_prompt_policy` plus the raw-capped and summary helpers so MCP and URL items are capped, truncated, and summarized exactly per config, including per-type prompt selection and summary token limits (src/simple_rag_writer/mcp/prompt_policy.py:10-112).
- `run_tasks` now delegates every reference through the shared policy function, ensuring both MCP and URL references respect `default_mode`, overrides, and continue wrapping blobs with labels (src/simple_rag_writer/runner/run_tasks.py:1-110).
- Introduced focused tests using a fake registry to prove raw mode enforces item/char ceilings and summary mode uses the configured prompts/model/token cap (tests/test_mcp_prompt_policy.py:16-103).

Tests: `pytest`

Next steps:
1. Update docs/config examples if you want to advertise the new summary prompt behavior.
2. Manually run `srw run …` with representative tasks to inspect the prompt snippets end-to-end.
file update:
diff --git a/src/simple_rag_writer/mcp/prompt_policy.py b/src/simple_rag_writer/mcp/prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..292f8de2265f9f601ea80e009a26efd7dff50541
--- /dev/null
+++ b/src/simple_rag_writer/mcp/prompt_policy.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+from typing import List, Optional
+
+from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.llm.registry import ModelRegistry
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import ReferenceCommon
+
+DEFAULT_SUMMARY_PROMPT = (
+  "Summarize the following reference material into key bullet points, highlighting"
+  " actionable facts and important context."
+)
+
+
+def _format_item(item: NormalizedItem) -> str:
+  parts: List[str] = []
+  if item.title:
+    parts.append(f"Title: {item.title}")
+  if item.url:
+    parts.append(f"URL: {item.url}")
+  body = (item.body or item.snippet or "").strip()
+  if body:
+    parts.append(body)
+  elif item.metadata:
+    parts.append(str(item.metadata))
+  return "\n".join(parts).strip()
+
+
+def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
+  if not items:
+    return None
+  per_item_limit = reference.max_chars or policy.max_chars_per_item
+  total_limit = policy.max_total_chars
+  max_items = reference.max_items or policy.max_items_per_reference
+  chunks: List[str] = []
+  used = 0
+  for item in items[:max_items]:
+    formatted = _format_item(item)
+    if not formatted:
+      continue
+    snippet = formatted[:per_item_limit].strip()
+    if not snippet:
+      continue
+    if used + len(snippet) > total_limit:
+      remaining = total_limit - used
+      if remaining <= 0:
+        break
+      snippet = snippet[:remaining].rstrip()
+    chunks.append(snippet)
+    used += len(snippet)
+    if used >= total_limit:
+      break
+  if not chunks:
+    return None
+  return "\n\n".join(chunks)
+
+
+def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
+  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
+  template = None
+  if ref_type:
+    template = policy.per_type_prompts.get(ref_type)
+  if not template:
+    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
+  sections: List[str] = []
+  for idx, item in enumerate(items, start=1):
+    heading = item.title or f"Item {idx}"
+    body = (item.body or item.snippet or "").strip()
+    lines = [f"## {heading}"]
+    if body:
+      lines.append(body)
+    if item.url:
+      lines.append(f"Source: {item.url}")
+    sections.append("\n".join(lines).strip())
+  body_text = "\n\n".join(sections)
+  return f"{template.strip()}\n\n{body_text}".strip()
+
+
+def _render_summary_blob(
+  reference: ReferenceCommon,
+  items: List[NormalizedItem],
+  policy: SummaryPolicy,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  max_items = reference.max_items or policy.max_items_per_reference
+  limited = items[:max_items]
+  prompt = _build_summary_prompt(reference, limited, policy)
+  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
+  summary = registry.complete(
+    prompt,
+    model_id=policy.summarizer_model,
+    task_params={"max_tokens": max_tokens},
+  )
+  return summary.strip() or None
+
+
+def apply_prompt_policy(
+  config: AppConfig,
+  items: List[NormalizedItem],
+  reference: ReferenceCommon,
+  registry: ModelRegistry,
+) -> Optional[str]:
+  if not items:
+    return None
+  policy = config.mcp_prompt_policy
+  mode = reference.prompt_mode or policy.default_mode
+  if mode == "summary":
+    return _render_summary_blob(reference, items, policy.summary, registry)
+  return _render_raw_blob(reference, items, policy.raw_capped)
diff --git a/src/simple_rag_writer/runner/run_tasks.py b/src/simple_rag_writer/runner/run_tasks.py
index d9fa29487a87ab4f79ef8dbcba52e97c779860b3..1e9005282ba5dc0d9f620b2be179331004c27883
--- a/src/simple_rag_writer/runner/run_tasks.py
+++ b/src/simple_rag_writer/runner/run_tasks.py
@@ -6,10 +6,11 @@
 
 from rich.console import Console
 
-from simple_rag_writer.config.models import AppConfig, RawCappedPolicy, SummaryPolicy
+from simple_rag_writer.config.models import AppConfig
 from simple_rag_writer.llm.registry import ModelRegistry
 from simple_rag_writer.mcp.client import McpClient
 from simple_rag_writer.mcp.normalization import normalize_payload
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
 from simple_rag_writer.mcp.types import NormalizedItem
 from simple_rag_writer.prompts.task_prompt import build_task_prompt
 from simple_rag_writer.runner.url_fetcher import fetch_url_text
@@ -17,114 +18,14 @@
 from simple_rag_writer.tasks.models import McpReference, ReferenceCommon, UrlReference
 
 console = Console()
-DEFAULT_SUMMARY_PROMPT = (
-  "Summarize the following reference material into key bullet points, highlighting"
-  " actionable facts and important context."
-)
 
 
 @dataclass
 class RunTasksResult:
   ok: bool
   failed_tasks: List[str]
-
-
-def _format_item(item: NormalizedItem) -> str:
-  parts: List[str] = []
-  if item.title:
-    parts.append(f"Title: {item.title}")
-  if item.url:
-    parts.append(f"URL: {item.url}")
-  body = (item.body or item.snippet or "").strip()
-  if body:
-    parts.append(body)
-  elif item.metadata:
-    parts.append(str(item.metadata))
-  return "\n".join(parts).strip()
-
-
-def _render_raw_blob(reference: ReferenceCommon, items: List[NormalizedItem], policy: RawCappedPolicy) -> Optional[str]:
-  if not items:
-    return None
-  per_item_limit = reference.max_chars or policy.max_chars_per_item
-  total_limit = policy.max_total_chars
-  max_items = reference.max_items or policy.max_items_per_reference
-  chunks: List[str] = []
-  used = 0
-  for item in items[:max_items]:
-    formatted = _format_item(item)
-    if not formatted:
-      continue
-    snippet = formatted[:per_item_limit].strip()
-    if not snippet:
-      continue
-    if used + len(snippet) > total_limit:
-      remaining = total_limit - used
-      if remaining <= 0:
-        break
-      snippet = snippet[:remaining].rstrip()
-    chunks.append(snippet)
-    used += len(snippet)
-    if used >= total_limit:
-      break
-  if not chunks:
-    return None
-  return "\n\n".join(chunks)
 
 
-def _build_summary_prompt(reference: ReferenceCommon, items: List[NormalizedItem], policy: SummaryPolicy) -> str:
-  ref_type = reference.item_type or next((itm.type for itm in items if itm.type), None)
-  template = None
-  if ref_type:
-    template = policy.per_type_prompts.get(ref_type)
-  if not template:
-    template = policy.default_prompt or DEFAULT_SUMMARY_PROMPT
-  sections: List[str] = []
-  for idx, item in enumerate(items, start=1):
-    heading = item.title or f"Item {idx}"
-    body = (item.body or item.snippet or "").strip()
-    lines = [f"## {heading}"]
-    if body:
-      lines.append(body)
-    if item.url:
-      lines.append(f"Source: {item.url}")
-    sections.append("\n".join(lines).strip())
-  body_text = "\n\n".join(sections)
-  return f"{template.strip()}\n\n{body_text}".strip()
-
-
-def _render_summary_blob(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  policy: SummaryPolicy,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  if not items:
-    return None
-  max_items = reference.max_items or policy.max_items_per_reference
-  limited = items[:max_items]
-  prompt = _build_summary_prompt(reference, limited, policy)
-  max_tokens = reference.summary_max_tokens or policy.summary_max_tokens
-  summary = registry.complete(
-    prompt,
-    model_id=policy.summarizer_model,
-    task_params={"max_tokens": max_tokens},
-  )
-  return summary.strip() or None
-
-
-def _apply_prompt_policy(
-  reference: ReferenceCommon,
-  items: List[NormalizedItem],
-  config: AppConfig,
-  registry: ModelRegistry,
-) -> Optional[str]:
-  mode = reference.prompt_mode or config.mcp_prompt_policy.default_mode
-  if mode == "summary":
-    return _render_summary_blob(reference, items, config.mcp_prompt_policy.summary, registry)
-  return _render_raw_blob(reference, items, config.mcp_prompt_policy.raw_capped)
-
-
 def _wrap_reference_label(reference: ReferenceCommon, text: Optional[str]) -> Optional[str]:
   if not text:
     return None
@@ -174,7 +75,7 @@
             console.print(f"[yellow]MCP error (skipping reference): {exc}[/yellow]")
             continue
           items = normalize_payload(result.payload, item_type_hint=ref.item_type)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
@@ -189,7 +90,7 @@
           if not fetched.strip():
             continue
           items = _url_items(ref, fetched)
-          blob = _apply_prompt_policy(ref, items, config, registry)
+          blob = apply_prompt_policy(config, items, ref, registry)
           wrapped = _wrap_reference_label(ref, blob)
           if wrapped:
             reference_blobs.append(wrapped)
diff --git a/tests/test_mcp_prompt_policy.py b/tests/test_mcp_prompt_policy.py
new file mode 100644
index 0000000000000000000000000000000000000000..d29583ab20d64aa6edbcdfd36d70c0cede922adf
--- /dev/null
+++ b/tests/test_mcp_prompt_policy.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+from simple_rag_writer.config.models import (
+  AppConfig,
+  McpPromptPolicy,
+  ModelConfig,
+  ProviderConfig,
+  RawCappedPolicy,
+  SummaryPolicy,
+)
+from simple_rag_writer.mcp.prompt_policy import apply_prompt_policy
+from simple_rag_writer.mcp.types import NormalizedItem
+from simple_rag_writer.tasks.models import McpReference
+
+
+class FakeModelRegistry:
+  def __init__(self, response: str = "SUMMARY OUTPUT") -> None:
+    self.response = response
+    self.calls = []
+
+  def complete(self, prompt: str, model_id: str | None = None, task_params=None) -> str:
+    self.calls.append(
+      {
+        "prompt": prompt,
+        "model_id": model_id,
+        "task_params": task_params,
+      }
+    )
+    return self.response
+
+
+def _make_config() -> AppConfig:
+  return AppConfig(
+    default_model="writer-model",
+    providers={
+      "local": ProviderConfig(type="openai", api_key="dummy"),
+    },
+    models=[
+      ModelConfig(id="writer-model", provider="local", model_name="writer"),
+      ModelConfig(id="summary-model", provider="local", model_name="summary"),
+    ],
+    mcp_prompt_policy=McpPromptPolicy(
+      default_mode="raw_capped",
+      raw_capped=RawCappedPolicy(
+        max_items_per_reference=2,
+        max_chars_per_item=5,
+        max_total_chars=9,
+      ),
+      summary=SummaryPolicy(
+        summarizer_model="summary-model",
+        max_items_per_reference=3,
+        summary_max_tokens=128,
+        default_prompt="Default summary prompt.",
+        per_type_prompts={
+          "case": "Summarize the legal case evidence.",
+        },
+      ),
+    ),
+  )
+
+
+def test_apply_prompt_policy_raw_capped_enforces_limits_and_defaults():
+  config = _make_config()
+  registry = FakeModelRegistry()
+  reference = McpReference(server="notes", tool="search")
+  items = [
+    NormalizedItem(body="AAAAA11111"),
+    NormalizedItem(body="BBBBB22222"),
+    NormalizedItem(body="CCCCC33333"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "AAAAA\n\nBBBB"
+  assert registry.calls == []
+
+
+def test_apply_prompt_policy_summary_uses_per_type_prompt_and_limits_items():
+  config = _make_config()
+  registry = FakeModelRegistry(response="  SUMMARY TEXT  ")
+  reference = McpReference(
+    server="notes",
+    tool="search",
+    prompt_mode="summary",
+    item_type="case",
+    summary_max_tokens=42,
+    max_items=1,
+  )
+  items = [
+    NormalizedItem(title="Doc 1", body="Alpha body", url="https://example.com/a"),
+    NormalizedItem(title="Doc 2", body="Beta body"),
+  ]
+
+  text = apply_prompt_policy(config, items, reference, registry)
+
+  assert text == "SUMMARY TEXT"
+  assert len(registry.calls) == 1
+  call = registry.calls[0]
+  assert call["model_id"] == "summary-model"
+  assert call["task_params"] == {"max_tokens": 42}
+  assert "Summarize the legal case evidence." in call["prompt"]
+  assert "Doc 1" in call["prompt"]
+  assert "Doc 2" not in call["prompt"]

tokens used
51,441
Commit message: feat: implement mcp prompt policy modes

Modified files:
- src/simple_rag_writer/mcp/prompt_policy.py
- src/simple_rag_writer/runner/run_tasks.py
- tests/test_mcp_prompt_policy.py

- Added `apply_prompt_policy` plus the raw-capped and summary helpers so MCP and URL items are capped, truncated, and summarized exactly per config, including per-type prompt selection and summary token limits (src/simple_rag_writer/mcp/prompt_policy.py:10-112).
- `run_tasks` now delegates every reference through the shared policy function, ensuring both MCP and URL references respect `default_mode`, overrides, and continue wrapping blobs with labels (src/simple_rag_writer/runner/run_tasks.py:1-110).
- Introduced focused tests using a fake registry to prove raw mode enforces item/char ceilings and summary mode uses the configured prompts/model/token cap (tests/test_mcp_prompt_policy.py:16-103).

Tests: `pytest`

Next steps:
1. Update docs/config examples if you want to advertise the new summary prompt behavior.
2. Manually run `srw run …` with representative tasks to inspect the prompt snippets end-to-end.
